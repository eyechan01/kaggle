{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72bc86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "231979cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "y_test = pd.read_csv(\"sample_submission.csv\")\n",
    "nrows = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ed35ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df8a445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Mean      Variance  Quartile1    Median  Quartile3\n",
      "Id                730.500000  1.777550e+05     365.75     730.5    1095.25\n",
      "MSSubClass         56.897260  1.789338e+03      20.00      50.0      70.00\n",
      "LotFrontage        70.049958  5.897492e+02      59.00      69.0      80.00\n",
      "LotArea         10516.828082  9.962565e+07    7553.50    9478.5   11601.50\n",
      "OverallQual         6.099315  1.912679e+00       5.00       6.0       7.00\n",
      "OverallCond         5.575342  1.238322e+00       5.00       5.0       6.00\n",
      "YearBuilt        1971.267808  9.122154e+02    1954.00    1973.0    2000.00\n",
      "YearRemodAdd     1984.865753  4.262328e+02    1967.00    1994.0    2004.00\n",
      "MasVnrArea        103.685262  3.278497e+04       0.00       0.0     166.00\n",
      "BsmtFinSF1        443.639726  2.080255e+05       0.00     383.5     712.25\n",
      "BsmtFinSF2         46.549315  2.602391e+04       0.00       0.0       0.00\n",
      "BsmtUnfSF         567.240411  1.952464e+05     223.00     477.5     808.00\n",
      "TotalBsmtSF      1057.429452  1.924624e+05     795.75     991.5    1298.25\n",
      "1stFlrSF         1162.626712  1.494501e+05     882.00    1087.0    1391.25\n",
      "2ndFlrSF          346.992466  1.905571e+05       0.00       0.0     728.00\n",
      "LowQualFinSF        5.844521  2.364204e+03       0.00       0.0       0.00\n",
      "GrLivArea        1515.463699  2.761296e+05    1129.50    1464.0    1776.75\n",
      "BsmtFullBath        0.425342  2.692682e-01       0.00       0.0       1.00\n",
      "BsmtHalfBath        0.057534  5.700283e-02       0.00       0.0       0.00\n",
      "FullBath            1.565068  3.035082e-01       1.00       2.0       2.00\n",
      "HalfBath            0.382877  2.528937e-01       0.00       0.0       1.00\n",
      "BedroomAbvGr        2.866438  6.654938e-01       2.00       3.0       3.00\n",
      "KitchenAbvGr        1.046575  4.854892e-02       1.00       1.0       1.00\n",
      "TotRmsAbvGrd        6.517808  2.641903e+00       5.00       6.0       7.00\n",
      "Fireplaces          0.613014  4.155947e-01       0.00       1.0       1.00\n",
      "GarageYrBlt      1978.506164  6.095825e+02    1961.00    1980.0    2002.00\n",
      "GarageCars          1.767123  5.584797e-01       1.00       2.0       2.00\n",
      "GarageArea        472.980137  4.571251e+04     334.50     480.0     576.00\n",
      "WoodDeckSF         94.244521  1.570981e+04       0.00       0.0     168.00\n",
      "OpenPorchSF        46.660274  4.389861e+03       0.00      25.0      68.00\n",
      "EnclosedPorch      21.954110  3.735550e+03       0.00       0.0       0.00\n",
      "3SsnPorch           3.409589  8.595059e+02       0.00       0.0       0.00\n",
      "ScreenPorch        15.060959  3.108889e+03       0.00       0.0       0.00\n",
      "PoolArea            2.758904  1.614216e+03       0.00       0.0       0.00\n",
      "MiscVal            43.489041  2.461381e+05       0.00       0.0       0.00\n",
      "MoSold              6.321918  7.309595e+00       5.00       6.0       8.00\n",
      "YrSold           2007.815753  1.763837e+00    2007.00    2008.0    2009.00\n",
      "SalePrice      180921.195890  6.311111e+09  129975.00  163000.0  214000.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/f4f083112sscgnyk35l8yjgh0000gn/T/ipykernel_43623/694865173.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  eda[\"Mean\"] = train.mean()\n",
      "/var/folders/jp/f4f083112sscgnyk35l8yjgh0000gn/T/ipykernel_43623/694865173.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  eda[\"Variance\"] = train.var()\n"
     ]
    }
   ],
   "source": [
    "eda = pd.DataFrame(columns = [\"Mean\", \"Variance\", \"Quartile1\", \"Median\", \"Quartile3\"])\n",
    "eda[\"Mean\"] = train.mean()\n",
    "eda[\"Variance\"] = train.var()\n",
    "eda[\"Quartile1\"] = train.quantile(0.25)\n",
    "eda[\"Median\"] = train.quantile(0.5)\n",
    "eda[\"Quartile3\"] = train.quantile(0.75)\n",
    "print(eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b7aa2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toDrop = []\n",
    "for i in range(len(train.columns)):\n",
    "    if train[train.columns[i]].isna().sum() > 0.3*nrows:\n",
    "        toDrop.append(train.columns[i])\n",
    "toDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37fb4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "naAllowed = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "            \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\",\n",
    "            \"Fence\", \"MiscFeature\"]\n",
    "for allowed in naAllowed:\n",
    "    train[allowed] = train[allowed].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f245793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LotFrontage', 259), ('MasVnrType', 8), ('MasVnrArea', 8), ('Electrical', 1), ('GarageYrBlt', 81)]\n"
     ]
    }
   ],
   "source": [
    "colHasNA = []\n",
    "for i in range(len(train.columns)):\n",
    "    if train[train.columns[i]].isna().sum() > 0:\n",
    "        colHasNA.append((train.columns[i], train[train.columns[i]].isna().sum()))\n",
    "print(colHasNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b09b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got rid of missing values\n",
    "trainClean = train.dropna()\n",
    "y_train = trainClean[\"SalePrice\"]\n",
    "trainClean = trainClean.drop(\"SalePrice\", axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43ad057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainClean.index = np.arange(0, len(trainClean), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74cc6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of the year variables, once OHE'd will be sparse, so grouping them\n",
    "yearVars = [\"YrSold\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\"]\n",
    "for var in yearVars:\n",
    "    trainClean[var + \"_simplified\"] = trainClean[var] - trainClean[var] % 10\n",
    "    trainClean = trainClean.drop(var, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58a3b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4    5    6    7    8    9    ...  314  315  316  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "1115  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1116  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1117  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "1118  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1119  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "      317  318  319  320  321  322  323  \n",
      "0     0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "1     0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "2     0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "3     0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4     0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "1115  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1116  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "1117  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1118  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1119  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1120 rows x 324 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>...</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 353 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  LotFrontage  LotArea  OverallQual  OverallCond  MasVnrArea  BsmtFinSF1  \\\n",
       "0   1         65.0     8450            7            5       196.0         706   \n",
       "1   2         80.0     9600            6            8         0.0         978   \n",
       "2   3         68.0    11250            7            5       162.0         486   \n",
       "3   4         60.0     9550            7            5         0.0         216   \n",
       "4   5         84.0    14260            8            5       350.0         655   \n",
       "\n",
       "   BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  ...  314  315  316  317  318  319  320  \\\n",
       "0           0        150          856  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1           0        284         1262  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
       "2           0        434          920  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3           0        540          756  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4           0        490         1145  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   321  322  323  \n",
       "0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  0.0  \n",
       "2  0.0  1.0  0.0  \n",
       "3  1.0  0.0  0.0  \n",
       "4  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 353 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OHE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "numericVars = set([\"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\",\n",
    "               \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\n",
    "               \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n",
    "               \"GrLivArea\", \"BsmtFullBath\", \"BsmtHalfBath\", \"FullBath\", \n",
    "               \"HalfBath\", \"Bedroom\", \"Kitchen\", \"TotRmsAbvGrd\",\n",
    "               \"Fireplaces\", \"GarageCars\", \"GarageArea\", \"WoodDeckSF\",\n",
    "               \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\",\n",
    "               \"PoolArea\", \"MiscVal\", \"SalePrice\"])\n",
    "\n",
    "theOriginal = trainClean.columns\n",
    "# for i in range(1, len(theOriginal)):\n",
    "#     currCol = theOriginal[i]\n",
    "#     if currCol not in numericVars and currCol not in yearVars:\n",
    "#         one_hot = pd.get_dummies(trainClean[currCol])\n",
    "#         dOHE[currCol] = one_hot.columns\n",
    "#         one_hot = one_hot.add_suffix('_' + currCol)\n",
    "#         trainClean = trainClean.drop(currCol, axis = 1)\n",
    "#         trainClean = pd.concat([trainClean, one_hot], axis = 1)\n",
    "\n",
    "categoricals = pd.DataFrame()\n",
    "for i in range(1, len(theOriginal)):\n",
    "    currCol = theOriginal[i]\n",
    "    if currCol not in numericVars and currCol not in yearVars:\n",
    "        categoricals[currCol] = trainClean[currCol]\n",
    "        trainClean = trainClean.drop(currCol, axis = 1)\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "ohe.fit(categoricals)\n",
    "new_categories = pd.DataFrame(ohe.transform(categoricals).toarray())\n",
    "print(new_categories)\n",
    "trainClean = trainClean.join(new_categories)\n",
    "trainClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6da0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LowQualFinSF\n",
      "17\n",
      "BsmtHalfBath\n",
      "61\n",
      "3SsnPorch\n",
      "18\n",
      "ScreenPorch\n",
      "95\n",
      "PoolArea\n",
      "6\n",
      "MiscVal\n",
      "34\n",
      "1\n",
      "47\n",
      "2\n",
      "3\n",
      "3\n",
      "9\n",
      "6\n",
      "52\n",
      "7\n",
      "13\n",
      "8\n",
      "37\n",
      "9\n",
      "14\n",
      "10\n",
      "35\n",
      "11\n",
      "66\n",
      "12\n",
      "53\n",
      "13\n",
      "6\n",
      "14\n",
      "19\n",
      "15\n",
      "8\n",
      "16\n",
      "54\n",
      "17\n",
      "10\n",
      "20\n",
      "4\n",
      "22\n",
      "41\n",
      "24\n",
      "36\n",
      "26\n",
      "26\n",
      "27\n",
      "7\n",
      "29\n",
      "47\n",
      "30\n",
      "44\n",
      "31\n",
      "15\n",
      "35\n",
      "44\n",
      "36\n",
      "30\n",
      "37\n",
      "4\n",
      "40\n",
      "44\n",
      "41\n",
      "5\n",
      "42\n",
      "14\n",
      "43\n",
      "2\n",
      "44\n",
      "15\n",
      "45\n",
      "42\n",
      "46\n",
      "12\n",
      "48\n",
      "41\n",
      "49\n",
      "71\n",
      "50\n",
      "49\n",
      "51\n",
      "27\n",
      "52\n",
      "10\n",
      "53\n",
      "32\n",
      "55\n",
      "7\n",
      "56\n",
      "45\n",
      "57\n",
      "33\n",
      "58\n",
      "75\n",
      "59\n",
      "97\n",
      "60\n",
      "19\n",
      "61\n",
      "46\n",
      "62\n",
      "48\n",
      "63\n",
      "75\n",
      "64\n",
      "20\n",
      "65\n",
      "29\n",
      "66\n",
      "7\n",
      "67\n",
      "43\n",
      "68\n",
      "56\n",
      "70\n",
      "4\n",
      "71\n",
      "9\n",
      "72\n",
      "9\n",
      "73\n",
      "24\n",
      "74\n",
      "1\n",
      "75\n",
      "4\n",
      "76\n",
      "2\n",
      "77\n",
      "5\n",
      "79\n",
      "1\n",
      "80\n",
      "2\n",
      "81\n",
      "2\n",
      "83\n",
      "20\n",
      "84\n",
      "35\n",
      "85\n",
      "35\n",
      "86\n",
      "90\n",
      "88\n",
      "10\n",
      "90\n",
      "6\n",
      "91\n",
      "9\n",
      "93\n",
      "24\n",
      "94\n",
      "43\n",
      "95\n",
      "6\n",
      "97\n",
      "10\n",
      "99\n",
      "6\n",
      "100\n",
      "1\n",
      "102\n",
      "1\n",
      "103\n",
      "1\n",
      "104\n",
      "6\n",
      "105\n",
      "2\n",
      "106\n",
      "6\n",
      "107\n",
      "14\n",
      "108\n",
      "2\n",
      "109\n",
      "40\n",
      "110\n",
      "1\n",
      "111\n",
      "45\n",
      "113\n",
      "1\n",
      "115\n",
      "68\n",
      "116\n",
      "1\n",
      "117\n",
      "19\n",
      "120\n",
      "22\n",
      "121\n",
      "15\n",
      "122\n",
      "2\n",
      "123\n",
      "7\n",
      "124\n",
      "21\n",
      "125\n",
      "1\n",
      "126\n",
      "45\n",
      "128\n",
      "9\n",
      "130\n",
      "1\n",
      "131\n",
      "87\n",
      "132\n",
      "4\n",
      "133\n",
      "20\n",
      "136\n",
      "29\n",
      "137\n",
      "9\n",
      "141\n",
      "46\n",
      "142\n",
      "7\n",
      "145\n",
      "2\n",
      "146\n",
      "17\n",
      "152\n",
      "15\n",
      "153\n",
      "6\n",
      "154\n",
      "2\n",
      "156\n",
      "32\n",
      "158\n",
      "24\n",
      "160\n",
      "36\n",
      "161\n",
      "51\n",
      "162\n",
      "24\n",
      "163\n",
      "1\n",
      "166\n",
      "97\n",
      "167\n",
      "89\n",
      "168\n",
      "25\n",
      "173\n",
      "56\n",
      "174\n",
      "24\n",
      "177\n",
      "13\n",
      "178\n",
      "25\n",
      "179\n",
      "12\n",
      "180\n",
      "35\n",
      "181\n",
      "25\n",
      "182\n",
      "37\n",
      "185\n",
      "17\n",
      "186\n",
      "2\n",
      "187\n",
      "1\n",
      "188\n",
      "3\n",
      "190\n",
      "30\n",
      "192\n",
      "1\n",
      "194\n",
      "63\n",
      "196\n",
      "70\n",
      "197\n",
      "20\n",
      "198\n",
      "2\n",
      "199\n",
      "1\n",
      "201\n",
      "4\n",
      "202\n",
      "28\n",
      "206\n",
      "14\n",
      "207\n",
      "4\n",
      "209\n",
      "46\n",
      "210\n",
      "1\n",
      "211\n",
      "91\n",
      "212\n",
      "22\n",
      "215\n",
      "10\n",
      "216\n",
      "4\n",
      "217\n",
      "24\n",
      "218\n",
      "26\n",
      "219\n",
      "12\n",
      "221\n",
      "21\n",
      "222\n",
      "24\n",
      "225\n",
      "13\n",
      "227\n",
      "5\n",
      "229\n",
      "15\n",
      "230\n",
      "64\n",
      "231\n",
      "8\n",
      "236\n",
      "3\n",
      "237\n",
      "46\n",
      "238\n",
      "11\n",
      "239\n",
      "3\n",
      "241\n",
      "2\n",
      "242\n",
      "32\n",
      "243\n",
      "5\n",
      "244\n",
      "6\n",
      "246\n",
      "51\n",
      "247\n",
      "23\n",
      "249\n",
      "2\n",
      "250\n",
      "2\n",
      "251\n",
      "2\n",
      "253\n",
      "47\n",
      "254\n",
      "39\n",
      "256\n",
      "10\n",
      "259\n",
      "1\n",
      "260\n",
      "34\n",
      "261\n",
      "1\n",
      "262\n",
      "44\n",
      "263\n",
      "43\n",
      "264\n",
      "75\n",
      "269\n",
      "94\n",
      "270\n",
      "49\n",
      "271\n",
      "74\n",
      "272\n",
      "60\n",
      "273\n",
      "41\n",
      "274\n",
      "32\n",
      "275\n",
      "4\n",
      "276\n",
      "2\n",
      "277\n",
      "6\n",
      "278\n",
      "3\n",
      "279\n",
      "4\n",
      "281\n",
      "1\n",
      "283\n",
      "71\n",
      "284\n",
      "1\n",
      "285\n",
      "9\n",
      "286\n",
      "18\n",
      "291\n",
      "7\n",
      "292\n",
      "6\n",
      "293\n",
      "13\n",
      "294\n",
      "40\n",
      "295\n",
      "78\n",
      "296\n",
      "42\n",
      "297\n",
      "58\n",
      "301\n",
      "41\n",
      "304\n",
      "1\n",
      "306\n",
      "96\n",
      "308\n",
      "50\n",
      "311\n",
      "5\n",
      "312\n",
      "3\n",
      "313\n",
      "13\n",
      "314\n",
      "48\n",
      "315\n",
      "36\n",
      "316\n",
      "51\n",
      "320\n",
      "72\n",
      "323\n",
      "3\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "# Checking Sparsity - we will need PCA\n",
    "\n",
    "# for i in range(len(train.columns)):\n",
    "#     if len(train[train.columns[i]].unique()) > 700:\n",
    "#         print(train.columns[i])\n",
    "count = 0\n",
    "for i in range(len(trainClean.columns)):\n",
    "    if np.count_nonzero(trainClean[trainClean.columns[i]]) < 100:\n",
    "        count += 1\n",
    "        print(trainClean.columns[i])\n",
    "        print(np.count_nonzero(trainClean[trainClean.columns[i]]))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49568e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Vars\n",
    "for i in range(len(trainClean.columns)):\n",
    "    if trainClean.columns[i] in numericVars:\n",
    "        minimum = trainClean[trainClean.columns[i]].min()\n",
    "        maximum = trainClean[trainClean.columns[i]].max()\n",
    "        if minimum != maximum:\n",
    "            trainClean[trainClean.columns[i]] = (trainClean[trainClean.columns[i]] - trainClean[trainClean.columns[i]].min())/(trainClean[trainClean.columns[i]].max() - trainClean[trainClean.columns[i]].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4db464a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AllPub_Utilities has only \"1\" values, so remove\n",
    "\n",
    "if \"AllPub_Utilities\" in trainClean.columns:\n",
    "    trainClean[\"AllPub_Utilities\"].unique()\n",
    "    trainClean = trainClean.drop(\"AllPub_Utilities\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "200cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning test data and OHE function\n",
    "        \n",
    "def cleanData(dataset):\n",
    "    if \"AllPub_Utilities\" in dataset.columns:\n",
    "        dataset = dataset.drop(\"AllPub_Utilities\", axis = 1)\n",
    "    for allowed in naAllowed:\n",
    "        dataset[allowed] = dataset[allowed].fillna(\"NA\")\n",
    "    for col in dataset.columns:\n",
    "        if col in numericVars:\n",
    "            dataset[col] = dataset[col].fillna(dataset[col].mean())\n",
    "        else:\n",
    "            if dataset[col].mode()[0]:\n",
    "                dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "            else:\n",
    "                dataset[col] = dataset[col].fillna(dataset[col].mode())\n",
    "    \n",
    "    for var in yearVars:\n",
    "        dataset[var + \"_simplified\"] = dataset[var] - dataset[var] % 10\n",
    "        dataset = dataset.drop(var, axis = 1)\n",
    "    \n",
    "    theOriginal = dataset.columns\n",
    "    categoricals = pd.DataFrame()\n",
    "    for i in range(1, len(theOriginal)):\n",
    "        currCol = theOriginal[i]\n",
    "        if currCol not in numericVars and currCol not in yearVars:\n",
    "            categoricals[currCol] = dataset[currCol]\n",
    "#             one_hot = pd.get_dummies(dataset[currCol])\n",
    "#             one_hot = one_hot.add_suffix('_' + currCol)\n",
    "            dataset = dataset.drop(currCol, axis = 1)\n",
    "#             dataset = pd.concat([dataset, one_hot], axis = 1)\n",
    "    new_categories = pd.DataFrame(ohe.transform(categoricals).toarray())\n",
    "    dataset = dataset.join(new_categories)\n",
    "    return dataset\n",
    "\n",
    "testClean = test\n",
    "testClean = cleanData(testClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8c10378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "[1.3233453545910647, 4.08184943205402, 3.314100629660007, 7.221797518016392, 3.6169538296757224, 3.502269645215476, inf, inf, inf, inf, inf, inf, inf, inf, 3.3256607737725363, 1.6363469225865317, 5.346881192423876, 3.799504402637922, 7.785614001253636, 9.415181498892604, 6.977720457779371, 7.2331041786888, 1.761459525291176, 2.0841319172956188, 2.194982215445369, 1.2476161033375195, 1.6461880283585608, 4422.62204884828, 5.672193093583222, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, <NA>, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]\n"
     ]
    }
   ],
   "source": [
    "# Testing Multicollinearity\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "trainClean.corr()\n",
    "\n",
    "vif_data = pd.DataFrame() \n",
    "vif_data[\"feature\"] = trainClean.columns \n",
    "result = []\n",
    "for i in range(len(trainClean.columns)):\n",
    "    # print(trainClean.columns[i])\n",
    "    if variance_inflation_factor(trainClean.values, i):\n",
    "        result.append(variance_inflation_factor(trainClean.values, i))\n",
    "    else:\n",
    "        print(trainClean.columns[i])\n",
    "        result.append(pd.NA)\n",
    "            \n",
    "vif_data[\"VIF\"] = result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1e9ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA + Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a9bc606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>...</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064212</td>\n",
       "      <td>0.140098</td>\n",
       "      <td>0.098260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121575</td>\n",
       "      <td>0.206547</td>\n",
       "      <td>0.193700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185788</td>\n",
       "      <td>0.150573</td>\n",
       "      <td>0.113305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231164</td>\n",
       "      <td>0.123732</td>\n",
       "      <td>0.122943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209760</td>\n",
       "      <td>0.187398</td>\n",
       "      <td>0.166197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LotFrontage   LotArea  OverallQual  OverallCond  MasVnrArea  BsmtFinSF1  \\\n",
       "0     0.150685  0.033420        0.625     0.428571     0.12250    0.125089   \n",
       "1     0.202055  0.038795        0.500     0.857143     0.00000    0.173281   \n",
       "2     0.160959  0.046507        0.625     0.428571     0.10125    0.086109   \n",
       "3     0.133562  0.038561        0.625     0.428571     0.00000    0.038271   \n",
       "4     0.215753  0.060576        0.750     0.428571     0.21875    0.116052   \n",
       "\n",
       "   BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  1stFlrSF  ...  314  315  316  317  318  \\\n",
       "0         0.0   0.064212     0.140098  0.098260  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "1         0.0   0.121575     0.206547  0.193700  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "2         0.0   0.185788     0.150573  0.113305  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "3         0.0   0.231164     0.123732  0.122943  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "4         0.0   0.209760     0.187398  0.166197  ...  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   319  320  321  322  323  \n",
       "0  0.0  0.0  0.0  1.0  0.0  \n",
       "1  1.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  1.0  0.0  \n",
       "3  0.0  0.0  1.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 352 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Id\n",
    "trainClean = trainClean.drop(\"Id\", axis = 1)\n",
    "trainClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4695950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14771054 0.05457183 0.04432251 0.0314231  0.02870551 0.02669806\n",
      " 0.02144991 0.01920427 0.01796188 0.01693464 0.01680209 0.01631585\n",
      " 0.01483606 0.0145168  0.01412279 0.01350514 0.01277671 0.0122449\n",
      " 0.01183557 0.01123567 0.01120126 0.01091972 0.01043213 0.00999059\n",
      " 0.00989149 0.00972076 0.00929389 0.00912191 0.00903004 0.00864853\n",
      " 0.00823526 0.00804705 0.00762676 0.00751966 0.00732084 0.0071455\n",
      " 0.00694862 0.00680565 0.00648765 0.00624151 0.00599439 0.0058767\n",
      " 0.00576507 0.00551129 0.00546795 0.00538869 0.0052067  0.00504177\n",
      " 0.00493223 0.00489977 0.00481758 0.0047371  0.0045153  0.00443304\n",
      " 0.00440702 0.00426475 0.004123   0.00404587 0.00395716 0.0038315\n",
      " 0.00382105 0.00375513 0.00361949 0.00351712 0.00343556 0.00335907\n",
      " 0.00328294 0.00318097 0.00317213 0.00304923 0.00304769 0.00299858\n",
      " 0.00287288 0.00286589 0.00273266 0.00272475 0.00264491 0.00260597\n",
      " 0.00244648 0.00237494]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components = 80)\n",
    "trainClean_pca = pca.fit_transform(trainClean)\n",
    "print(pca.explained_variance_ratio_)\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f9d75a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "testClean = testClean.drop(\"Id\", axis = 1)\n",
    "testClean_pca = pca.transform(testClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37d214f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SalePrice   R-squared:                       0.828\n",
      "Model:                            OLS   Adj. R-squared:                  0.815\n",
      "Method:                 Least Squares   F-statistic:                     62.45\n",
      "Date:                Sat, 06 Jan 2024   Prob (F-statistic):               0.00\n",
      "Time:                        20:30:54   Log-Likelihood:                -13290.\n",
      "No. Observations:                1120   AIC:                         2.674e+04\n",
      "Df Residuals:                    1039   BIC:                         2.715e+04\n",
      "Df Model:                          80                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.855e+05   1068.371    173.650      0.000    1.83e+05    1.88e+05\n",
      "x1          3.266e+04    616.726     52.957      0.000    3.14e+04    3.39e+04\n",
      "x2         -8666.6620   1014.644     -8.542      0.000   -1.07e+04   -6675.676\n",
      "x3          7109.2318   1125.864      6.314      0.000    4900.006    9318.458\n",
      "x4          4.354e+04   1337.130     32.562      0.000    4.09e+04    4.62e+04\n",
      "x5           998.4714   1398.993      0.714      0.476   -1746.702    3743.644\n",
      "x6          1.407e+04   1450.635      9.700      0.000    1.12e+04    1.69e+04\n",
      "x7          2378.6773   1618.398      1.470      0.142    -797.024    5554.379\n",
      "x8          3723.9966   1710.406      2.177      0.030     367.753    7080.240\n",
      "x9          5892.3834   1768.570      3.332      0.001    2422.008    9362.759\n",
      "x10         1.246e+04   1821.420      6.841      0.000    8887.072     1.6e+04\n",
      "x11        -4064.6457   1828.591     -2.223      0.026   -7652.797    -476.494\n",
      "x12         2472.7120   1855.638      1.333      0.183   -1168.513    6113.937\n",
      "x13        -7897.6675   1945.982     -4.058      0.000   -1.17e+04   -4079.165\n",
      "x14         1.879e+04   1967.264      9.551      0.000    1.49e+04    2.26e+04\n",
      "x15         2182.4520   1994.517      1.094      0.274   -1731.289    6096.193\n",
      "x16           81.5882   2039.616      0.040      0.968   -3920.649    4083.825\n",
      "x17        -1560.4777   2096.952     -0.744      0.457   -5675.222    2554.267\n",
      "x18        -2831.2438   2142.005     -1.322      0.187   -7034.393    1371.905\n",
      "x19          678.8472   2178.731      0.312      0.755   -3596.367    4954.061\n",
      "x20         4792.0421   2236.138      2.143      0.032     404.180    9179.904\n",
      "x21        -1.648e+04   2239.570     -7.361      0.000   -2.09e+04   -1.21e+04\n",
      "x22        -1287.2821   2268.257     -0.568      0.570   -5738.169    3163.605\n",
      "x23        -2027.7364   2320.661     -0.874      0.382   -6581.452    2525.979\n",
      "x24         -953.8012   2371.388     -0.402      0.688   -5607.056    3699.454\n",
      "x25        -5325.9208   2383.237     -2.235      0.026      -1e+04    -649.414\n",
      "x26         1.942e+04   2404.075      8.076      0.000    1.47e+04    2.41e+04\n",
      "x27         9723.1796   2458.665      3.955      0.000    4898.664    1.45e+04\n",
      "x28         1.419e+04   2481.734      5.717      0.000    9317.104    1.91e+04\n",
      "x29         8533.3312   2494.327      3.421      0.001    3638.839    1.34e+04\n",
      "x30         -756.0329   2548.748     -0.297      0.767   -5757.312    4245.246\n",
      "x31         2.174e+04   2611.918      8.323      0.000    1.66e+04    2.69e+04\n",
      "x32        -4106.3826   2642.286     -1.554      0.120   -9291.208    1078.442\n",
      "x33        -7997.7246   2714.115     -2.947      0.003   -1.33e+04   -2671.954\n",
      "x34        -3.017e+04   2733.374    -11.039      0.000   -3.55e+04   -2.48e+04\n",
      "x35          954.1177   2770.241      0.344      0.731   -4481.787    6390.022\n",
      "x36         1.924e+04   2804.024      6.860      0.000    1.37e+04    2.47e+04\n",
      "x37        -1.234e+04   2843.470     -4.339      0.000   -1.79e+04   -6758.484\n",
      "x38         1991.5083   2873.183      0.693      0.488   -3646.394    7629.411\n",
      "x39        -6397.1499   2942.756     -2.174      0.030   -1.22e+04    -622.728\n",
      "x40         6921.7992   3000.220      2.307      0.021    1034.618    1.28e+04\n",
      "x41         9133.2691   3061.440      2.983      0.003    3125.960    1.51e+04\n",
      "x42         2958.1665   3091.941      0.957      0.339   -3108.995    9025.328\n",
      "x43         1150.5015   3121.734      0.369      0.713   -4975.120    7276.123\n",
      "x44         -368.6731   3192.799     -0.115      0.908   -6633.741    5896.395\n",
      "x45         1453.7419   3205.427      0.454      0.650   -4836.106    7743.590\n",
      "x46        -8738.0586   3228.913     -2.706      0.007   -1.51e+04   -2402.124\n",
      "x47        -7221.5369   3284.859     -2.198      0.028   -1.37e+04    -775.824\n",
      "x48        -6726.9477   3338.156     -2.015      0.044   -1.33e+04    -176.653\n",
      "x49         9672.0645   3375.021      2.866      0.004    3049.431    1.63e+04\n",
      "x50         1408.7533   3386.181      0.416      0.677   -5235.780    8053.287\n",
      "x51        -1.481e+04   3414.943     -4.337      0.000   -2.15e+04   -8108.300\n",
      "x52        -1.633e+04   3443.829     -4.740      0.000   -2.31e+04   -9567.535\n",
      "x53         2.524e+04   3527.400      7.156      0.000    1.83e+04    3.22e+04\n",
      "x54          759.7309   3559.977      0.213      0.831   -6225.833    7745.295\n",
      "x55        -1.011e+04   3570.474     -2.832      0.005   -1.71e+04   -3106.934\n",
      "x56         4804.4673   3629.540      1.324      0.186   -2317.596    1.19e+04\n",
      "x57         3629.7363   3691.401      0.983      0.326   -3613.714    1.09e+04\n",
      "x58         1054.1961   3726.423      0.283      0.777   -6257.976    8366.368\n",
      "x59         5661.4050   3767.962      1.503      0.133   -1732.277    1.31e+04\n",
      "x60        -1.667e+04   3829.247     -4.352      0.000   -2.42e+04   -9152.260\n",
      "x61        -6285.8665   3834.483     -1.639      0.101   -1.38e+04    1238.347\n",
      "x62        -1.554e+04   3867.992     -4.017      0.000   -2.31e+04   -7946.288\n",
      "x63         1463.0646   3939.804      0.371      0.710   -6267.815    9193.944\n",
      "x64        -7294.6714   3996.726     -1.825      0.068   -1.51e+04     547.903\n",
      "x65         1795.5729   4043.889      0.444      0.657   -6139.548    9730.693\n",
      "x66        -1.901e+04   4089.671     -4.647      0.000    -2.7e+04    -1.1e+04\n",
      "x67        -7809.7154   4136.817     -1.888      0.059   -1.59e+04     307.753\n",
      "x68         1948.5511   4202.605      0.464      0.643   -6298.011    1.02e+04\n",
      "x69        -5756.8853   4208.457     -1.368      0.172    -1.4e+04    2501.159\n",
      "x70         5181.0468   4292.427      1.207      0.228   -3241.767    1.36e+04\n",
      "x71         9873.2557   4293.513      2.300      0.022    1448.310    1.83e+04\n",
      "x72        -1677.5146   4328.527     -0.388      0.698   -1.02e+04    6816.137\n",
      "x73         4980.6813   4422.212      1.126      0.260   -3696.804    1.37e+04\n",
      "x74         5146.9762   4427.602      1.162      0.245   -3541.086    1.38e+04\n",
      "x75        -4041.9480   4534.246     -0.891      0.373   -1.29e+04    4855.376\n",
      "x76        -4315.0047   4540.826     -0.950      0.342   -1.32e+04    4595.230\n",
      "x77        -5421.9620   4608.853     -1.176      0.240   -1.45e+04    3621.758\n",
      "x78         9558.9192   4643.154      2.059      0.040     447.892    1.87e+04\n",
      "x79        -3.252e+04   4792.111     -6.787      0.000   -4.19e+04   -2.31e+04\n",
      "x80        -1209.7580   4863.755     -0.249      0.804   -1.08e+04    8334.145\n",
      "==============================================================================\n",
      "Omnibus:                      419.579   Durbin-Watson:                   1.934\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14423.132\n",
      "Skew:                           1.054   Prob(JB):                         0.00\n",
      "Kurtosis:                      20.453   Cond. No.                         7.89\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# LinReg Training\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "X2 = sm.add_constant(trainClean_pca)\n",
    "est = sm.OLS(y_train, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "633d2d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=10, max_iter=100000.0, normalize=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression Training\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'alpha' : [1, 2, 5, 10, 50, 100]\n",
    "}\n",
    "lasso = Lasso(alpha=1,normalize=True, max_iter=1e5)\n",
    "\n",
    "lasso_CV = GridSearchCV(estimator=lasso, param_grid=param_grid, n_jobs=-1, cv=None, verbose=1)\n",
    "lasso_CV.fit(trainClean_pca, y_train)\n",
    "print(lasso_CV.best_params_)\n",
    "\n",
    "lassoreg = Lasso(alpha=10,normalize=True, max_iter=1e5)\n",
    "lassoreg.fit(trainClean_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c7d596ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.234632</td>\n",
       "      <td>0.282263</td>\n",
       "      <td>0.018860</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 3, 'n_e...</td>\n",
       "      <td>0.760342</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.719748</td>\n",
       "      <td>0.801450</td>\n",
       "      <td>0.719870</td>\n",
       "      <td>0.752477</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.216268</td>\n",
       "      <td>0.082150</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 3, 'n_...</td>\n",
       "      <td>0.760342</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.719748</td>\n",
       "      <td>0.801450</td>\n",
       "      <td>0.719870</td>\n",
       "      <td>0.752477</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.469624</td>\n",
       "      <td>0.043699</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 3, 'n_e...</td>\n",
       "      <td>0.760342</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.719748</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>0.719870</td>\n",
       "      <td>0.752467</td>\n",
       "      <td>0.030535</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.217581</td>\n",
       "      <td>0.136858</td>\n",
       "      <td>0.039982</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 3, 'n_...</td>\n",
       "      <td>0.757609</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.723485</td>\n",
       "      <td>0.805903</td>\n",
       "      <td>0.711277</td>\n",
       "      <td>0.752135</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.029876</td>\n",
       "      <td>0.191453</td>\n",
       "      <td>0.038763</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 3, 'n_e...</td>\n",
       "      <td>0.757609</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.723485</td>\n",
       "      <td>0.805903</td>\n",
       "      <td>0.711277</td>\n",
       "      <td>0.752135</td>\n",
       "      <td>0.033216</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.127052</td>\n",
       "      <td>0.168510</td>\n",
       "      <td>0.034906</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 3, 'n_e...</td>\n",
       "      <td>0.757578</td>\n",
       "      <td>0.762402</td>\n",
       "      <td>0.723485</td>\n",
       "      <td>0.805895</td>\n",
       "      <td>0.711277</td>\n",
       "      <td>0.752127</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.522702</td>\n",
       "      <td>0.031796</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.758253</td>\n",
       "      <td>0.755756</td>\n",
       "      <td>0.719819</td>\n",
       "      <td>0.800963</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.749736</td>\n",
       "      <td>0.031347</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.708041</td>\n",
       "      <td>0.040670</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 5, 'n_...</td>\n",
       "      <td>0.758253</td>\n",
       "      <td>0.755756</td>\n",
       "      <td>0.719819</td>\n",
       "      <td>0.800963</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.749736</td>\n",
       "      <td>0.031347</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.147704</td>\n",
       "      <td>0.102270</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.758253</td>\n",
       "      <td>0.755756</td>\n",
       "      <td>0.719819</td>\n",
       "      <td>0.800963</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.749736</td>\n",
       "      <td>0.031347</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.873043</td>\n",
       "      <td>0.116164</td>\n",
       "      <td>0.032165</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.758747</td>\n",
       "      <td>0.760098</td>\n",
       "      <td>0.720422</td>\n",
       "      <td>0.804488</td>\n",
       "      <td>0.704544</td>\n",
       "      <td>0.749660</td>\n",
       "      <td>0.034894</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.148581</td>\n",
       "      <td>0.108165</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 5, 'n_...</td>\n",
       "      <td>0.758747</td>\n",
       "      <td>0.760098</td>\n",
       "      <td>0.720422</td>\n",
       "      <td>0.804484</td>\n",
       "      <td>0.704544</td>\n",
       "      <td>0.749659</td>\n",
       "      <td>0.034892</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.520491</td>\n",
       "      <td>0.119188</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.758747</td>\n",
       "      <td>0.760098</td>\n",
       "      <td>0.720422</td>\n",
       "      <td>0.804484</td>\n",
       "      <td>0.704544</td>\n",
       "      <td>0.749659</td>\n",
       "      <td>0.034892</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.420679</td>\n",
       "      <td>0.400217</td>\n",
       "      <td>0.018902</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.759595</td>\n",
       "      <td>0.758367</td>\n",
       "      <td>0.716194</td>\n",
       "      <td>0.801201</td>\n",
       "      <td>0.710716</td>\n",
       "      <td>0.749214</td>\n",
       "      <td>0.033065</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.422181</td>\n",
       "      <td>0.025992</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 5, 'n_e...</td>\n",
       "      <td>0.759595</td>\n",
       "      <td>0.758367</td>\n",
       "      <td>0.716194</td>\n",
       "      <td>0.801201</td>\n",
       "      <td>0.710716</td>\n",
       "      <td>0.749214</td>\n",
       "      <td>0.033065</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.878684</td>\n",
       "      <td>0.086268</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 5, 'n_...</td>\n",
       "      <td>0.759595</td>\n",
       "      <td>0.758367</td>\n",
       "      <td>0.716194</td>\n",
       "      <td>0.801201</td>\n",
       "      <td>0.710716</td>\n",
       "      <td>0.749214</td>\n",
       "      <td>0.033065</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.906165</td>\n",
       "      <td>0.075065</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 3, 'n_...</td>\n",
       "      <td>0.759459</td>\n",
       "      <td>0.753361</td>\n",
       "      <td>0.716411</td>\n",
       "      <td>0.803439</td>\n",
       "      <td>0.710216</td>\n",
       "      <td>0.748577</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.602045</td>\n",
       "      <td>0.019434</td>\n",
       "      <td>0.007120</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 3, 'n_e...</td>\n",
       "      <td>0.759459</td>\n",
       "      <td>0.753361</td>\n",
       "      <td>0.716411</td>\n",
       "      <td>0.803439</td>\n",
       "      <td>0.710216</td>\n",
       "      <td>0.748577</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.704759</td>\n",
       "      <td>0.044925</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 3, 'n_e...</td>\n",
       "      <td>0.759459</td>\n",
       "      <td>0.753361</td>\n",
       "      <td>0.716411</td>\n",
       "      <td>0.803439</td>\n",
       "      <td>0.710216</td>\n",
       "      <td>0.748577</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.263450</td>\n",
       "      <td>0.141690</td>\n",
       "      <td>0.032523</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.756935</td>\n",
       "      <td>0.759689</td>\n",
       "      <td>0.720359</td>\n",
       "      <td>0.801731</td>\n",
       "      <td>0.702761</td>\n",
       "      <td>0.748295</td>\n",
       "      <td>0.034391</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.069637</td>\n",
       "      <td>0.101443</td>\n",
       "      <td>0.033151</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.756935</td>\n",
       "      <td>0.759689</td>\n",
       "      <td>0.720359</td>\n",
       "      <td>0.801731</td>\n",
       "      <td>0.702761</td>\n",
       "      <td>0.748295</td>\n",
       "      <td>0.034391</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.662494</td>\n",
       "      <td>0.197718</td>\n",
       "      <td>0.022935</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 10, 'n...</td>\n",
       "      <td>0.756935</td>\n",
       "      <td>0.759689</td>\n",
       "      <td>0.720359</td>\n",
       "      <td>0.801731</td>\n",
       "      <td>0.702761</td>\n",
       "      <td>0.748295</td>\n",
       "      <td>0.034391</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.530861</td>\n",
       "      <td>0.113576</td>\n",
       "      <td>0.015157</td>\n",
       "      <td>0.002435</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 10, 'n...</td>\n",
       "      <td>0.756770</td>\n",
       "      <td>0.759861</td>\n",
       "      <td>0.716538</td>\n",
       "      <td>0.797108</td>\n",
       "      <td>0.710186</td>\n",
       "      <td>0.748093</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.001511</td>\n",
       "      <td>0.084111</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.003239</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.756770</td>\n",
       "      <td>0.759861</td>\n",
       "      <td>0.716538</td>\n",
       "      <td>0.797108</td>\n",
       "      <td>0.710186</td>\n",
       "      <td>0.748093</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.051130</td>\n",
       "      <td>0.181217</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>250</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.756770</td>\n",
       "      <td>0.759861</td>\n",
       "      <td>0.716538</td>\n",
       "      <td>0.797108</td>\n",
       "      <td>0.710186</td>\n",
       "      <td>0.748093</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.580036</td>\n",
       "      <td>0.057048</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.755966</td>\n",
       "      <td>0.758869</td>\n",
       "      <td>0.718833</td>\n",
       "      <td>0.799768</td>\n",
       "      <td>0.706156</td>\n",
       "      <td>0.747918</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.410456</td>\n",
       "      <td>0.057382</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'min_samples_split': 10, 'n_...</td>\n",
       "      <td>0.755966</td>\n",
       "      <td>0.758869</td>\n",
       "      <td>0.718833</td>\n",
       "      <td>0.799768</td>\n",
       "      <td>0.706156</td>\n",
       "      <td>0.747918</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.730832</td>\n",
       "      <td>0.042703</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 100, 'min_samples_split': 10, 'n...</td>\n",
       "      <td>0.755966</td>\n",
       "      <td>0.758869</td>\n",
       "      <td>0.718833</td>\n",
       "      <td>0.799768</td>\n",
       "      <td>0.706156</td>\n",
       "      <td>0.747918</td>\n",
       "      <td>0.033055</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10       7.234632      0.282263         0.018860        0.004896   \n",
       "19       7.216268      0.082150         0.017084        0.003101   \n",
       "1        6.469624      0.043699         0.019197        0.004497   \n",
       "20      14.217581      0.136858         0.039982        0.007997   \n",
       "11      16.029876      0.191453         0.038763        0.004393   \n",
       "2       13.127052      0.168510         0.034906        0.004198   \n",
       "3        2.522702      0.031796         0.007288        0.003382   \n",
       "21       2.708041      0.040670         0.006467        0.001746   \n",
       "12       3.147704      0.102270         0.006860        0.002281   \n",
       "5       12.873043      0.116164         0.032165        0.002802   \n",
       "23      14.148581      0.108165         0.039586        0.004637   \n",
       "14      14.520491      0.119188         0.035250        0.004572   \n",
       "13       7.420679      0.400217         0.018902        0.003476   \n",
       "4        6.422181      0.025992         0.015539        0.005034   \n",
       "22       6.878684      0.086268         0.018450        0.002465   \n",
       "18       2.906165      0.075065         0.009058        0.003008   \n",
       "0        2.602045      0.019434         0.007120        0.002575   \n",
       "9        2.704759      0.044925         0.009167        0.003119   \n",
       "17      13.263450      0.141690         0.032523        0.002068   \n",
       "8       12.069637      0.101443         0.033151        0.004023   \n",
       "26      11.662494      0.197718         0.022935        0.003008   \n",
       "25       6.530861      0.113576         0.015157        0.002435   \n",
       "7        6.001511      0.084111         0.014553        0.003239   \n",
       "16       7.051130      0.181217         0.015343        0.002536   \n",
       "15       2.580036      0.057048         0.006965        0.001282   \n",
       "6        2.410456      0.057382         0.008508        0.003394   \n",
       "24       2.730832      0.042703         0.008677        0.002863   \n",
       "\n",
       "   param_max_depth param_min_samples_split param_n_estimators  \\\n",
       "10              50                       3                250   \n",
       "19             100                       3                250   \n",
       "1               25                       3                250   \n",
       "20             100                       3                500   \n",
       "11              50                       3                500   \n",
       "2               25                       3                500   \n",
       "3               25                       5                100   \n",
       "21             100                       5                100   \n",
       "12              50                       5                100   \n",
       "5               25                       5                500   \n",
       "23             100                       5                500   \n",
       "14              50                       5                500   \n",
       "13              50                       5                250   \n",
       "4               25                       5                250   \n",
       "22             100                       5                250   \n",
       "18             100                       3                100   \n",
       "0               25                       3                100   \n",
       "9               50                       3                100   \n",
       "17              50                      10                500   \n",
       "8               25                      10                500   \n",
       "26             100                      10                500   \n",
       "25             100                      10                250   \n",
       "7               25                      10                250   \n",
       "16              50                      10                250   \n",
       "15              50                      10                100   \n",
       "6               25                      10                100   \n",
       "24             100                      10                100   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "10  {'max_depth': 50, 'min_samples_split': 3, 'n_e...           0.760342   \n",
       "19  {'max_depth': 100, 'min_samples_split': 3, 'n_...           0.760342   \n",
       "1   {'max_depth': 25, 'min_samples_split': 3, 'n_e...           0.760342   \n",
       "20  {'max_depth': 100, 'min_samples_split': 3, 'n_...           0.757609   \n",
       "11  {'max_depth': 50, 'min_samples_split': 3, 'n_e...           0.757609   \n",
       "2   {'max_depth': 25, 'min_samples_split': 3, 'n_e...           0.757578   \n",
       "3   {'max_depth': 25, 'min_samples_split': 5, 'n_e...           0.758253   \n",
       "21  {'max_depth': 100, 'min_samples_split': 5, 'n_...           0.758253   \n",
       "12  {'max_depth': 50, 'min_samples_split': 5, 'n_e...           0.758253   \n",
       "5   {'max_depth': 25, 'min_samples_split': 5, 'n_e...           0.758747   \n",
       "23  {'max_depth': 100, 'min_samples_split': 5, 'n_...           0.758747   \n",
       "14  {'max_depth': 50, 'min_samples_split': 5, 'n_e...           0.758747   \n",
       "13  {'max_depth': 50, 'min_samples_split': 5, 'n_e...           0.759595   \n",
       "4   {'max_depth': 25, 'min_samples_split': 5, 'n_e...           0.759595   \n",
       "22  {'max_depth': 100, 'min_samples_split': 5, 'n_...           0.759595   \n",
       "18  {'max_depth': 100, 'min_samples_split': 3, 'n_...           0.759459   \n",
       "0   {'max_depth': 25, 'min_samples_split': 3, 'n_e...           0.759459   \n",
       "9   {'max_depth': 50, 'min_samples_split': 3, 'n_e...           0.759459   \n",
       "17  {'max_depth': 50, 'min_samples_split': 10, 'n_...           0.756935   \n",
       "8   {'max_depth': 25, 'min_samples_split': 10, 'n_...           0.756935   \n",
       "26  {'max_depth': 100, 'min_samples_split': 10, 'n...           0.756935   \n",
       "25  {'max_depth': 100, 'min_samples_split': 10, 'n...           0.756770   \n",
       "7   {'max_depth': 25, 'min_samples_split': 10, 'n_...           0.756770   \n",
       "16  {'max_depth': 50, 'min_samples_split': 10, 'n_...           0.756770   \n",
       "15  {'max_depth': 50, 'min_samples_split': 10, 'n_...           0.755966   \n",
       "6   {'max_depth': 25, 'min_samples_split': 10, 'n_...           0.755966   \n",
       "24  {'max_depth': 100, 'min_samples_split': 10, 'n...           0.755966   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "10           0.760976           0.719748           0.801450   \n",
       "19           0.760976           0.719748           0.801450   \n",
       "1            0.760976           0.719748           0.801400   \n",
       "20           0.762402           0.723485           0.805903   \n",
       "11           0.762402           0.723485           0.805903   \n",
       "2            0.762402           0.723485           0.805895   \n",
       "3            0.755756           0.719819           0.800963   \n",
       "21           0.755756           0.719819           0.800963   \n",
       "12           0.755756           0.719819           0.800963   \n",
       "5            0.760098           0.720422           0.804488   \n",
       "23           0.760098           0.720422           0.804484   \n",
       "14           0.760098           0.720422           0.804484   \n",
       "13           0.758367           0.716194           0.801201   \n",
       "4            0.758367           0.716194           0.801201   \n",
       "22           0.758367           0.716194           0.801201   \n",
       "18           0.753361           0.716411           0.803439   \n",
       "0            0.753361           0.716411           0.803439   \n",
       "9            0.753361           0.716411           0.803439   \n",
       "17           0.759689           0.720359           0.801731   \n",
       "8            0.759689           0.720359           0.801731   \n",
       "26           0.759689           0.720359           0.801731   \n",
       "25           0.759861           0.716538           0.797108   \n",
       "7            0.759861           0.716538           0.797108   \n",
       "16           0.759861           0.716538           0.797108   \n",
       "15           0.758869           0.718833           0.799768   \n",
       "6            0.758869           0.718833           0.799768   \n",
       "24           0.758869           0.718833           0.799768   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "10           0.719870         0.752477        0.030551                1  \n",
       "19           0.719870         0.752477        0.030551                1  \n",
       "1            0.719870         0.752467        0.030535                3  \n",
       "20           0.711277         0.752135        0.033216                4  \n",
       "11           0.711277         0.752135        0.033216                4  \n",
       "2            0.711277         0.752127        0.033213                6  \n",
       "3            0.713888         0.749736        0.031347                7  \n",
       "21           0.713888         0.749736        0.031347                7  \n",
       "12           0.713888         0.749736        0.031347                7  \n",
       "5            0.704544         0.749660        0.034894               10  \n",
       "23           0.704544         0.749659        0.034892               11  \n",
       "14           0.704544         0.749659        0.034892               11  \n",
       "13           0.710716         0.749214        0.033065               13  \n",
       "4            0.710716         0.749214        0.033065               13  \n",
       "22           0.710716         0.749214        0.033065               13  \n",
       "18           0.710216         0.748577        0.033637               16  \n",
       "0            0.710216         0.748577        0.033637               16  \n",
       "9            0.710216         0.748577        0.033637               16  \n",
       "17           0.702761         0.748295        0.034391               19  \n",
       "8            0.702761         0.748295        0.034391               19  \n",
       "26           0.702761         0.748295        0.034391               19  \n",
       "25           0.710186         0.748093        0.031777               22  \n",
       "7            0.710186         0.748093        0.031777               22  \n",
       "16           0.710186         0.748093        0.031777               22  \n",
       "15           0.706156         0.747918        0.033055               25  \n",
       "6            0.706156         0.747918        0.033055               25  \n",
       "24           0.706156         0.747918        0.033055               25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'min_samples_split': 3, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Hyperparam Tuning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_grid = {'min_samples_split' : [3, 5, 10],\n",
    "              'max_depth' : [25, 50, 100],\n",
    "              'n_estimators' : [100, 250, 500]}\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "rf_CV = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=-1, cv=None, verbose=1)\n",
    "rf_CV.fit(trainClean_pca, y_train)\n",
    "\n",
    "rf_results = pd.DataFrame(rf_CV.cv_results_).sort_values(by=['rank_test_score'])\n",
    "display(rf_results)\n",
    "\n",
    "print(rf_CV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ceb7f0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=50, min_samples_split=3, n_estimators=250,\n",
       "                      n_jobs=-1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using optimal parameters in RF\n",
    "\n",
    "rfModel = RandomForestRegressor(n_estimators=250, max_depth = 50, min_samples_split = 3, n_jobs=-1)\n",
    "rfModel.fit(trainClean_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c1d761e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.018696022695571e+19\n",
      "9.156588116602081e+18\n",
      "261270059616852.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predict and Evaluate\n",
    "testClean_pca = pca.transform(testClean)\n",
    "\n",
    "# Make linear reg predictions\n",
    "testClean_pca_constant = sm.add_constant(testClean_pca)\n",
    "ypred_linreg = est2.predict(testClean_pca_constant)\n",
    "print(sum((ypred_linreg-y_test[\"SalePrice\"])**2))\n",
    "\n",
    "# Make lasso reg predictions\n",
    "y_pred = lassoreg.predict(testClean_pca)\n",
    "rss = sum((y_pred-y_test[\"SalePrice\"])**2)\n",
    "\n",
    "ret = [rss]\n",
    "ret.extend([lassoreg.intercept_])\n",
    "ret.extend(lassoreg.coef_)\n",
    "print(rss)\n",
    "\n",
    "# make our RF predictions\n",
    "rf_preds = rfModel.predict(testClean_pca)\n",
    "print(sum((y_test[\"SalePrice\"]-rf_preds)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e0b1d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.156588116602081e+18\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create regression matrices\n",
    "dtrain_reg = xgb.DMatrix(trainClean_pca, y_train, enable_categorical=True)\n",
    "dtest_reg = xgb.DMatrix(testClean_pca, y_test[\"SalePrice\"], enable_categorical=True)\n",
    "params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"hist\"}\n",
    "\n",
    "n = 100\n",
    "model = xgb.train(\n",
    "   params=params,\n",
    "   dtrain=dtrain_reg,\n",
    "   num_boost_round=n,\n",
    ")\n",
    "\n",
    "preds = model.predict(dtest_reg)\n",
    "print(sum((y_pred-y_test[\"SalePrice\"])**2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
